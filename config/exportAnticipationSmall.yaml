model:
  type: gpt2
  hidden_dim: 768
  num_heads: 12
  num_layers: 12
  seq_len: 1024
  scale_attn_by_inverse_layer_idx: true
  gradient_checkpointing: true
  use_flash_attention: false

override_vocab_size: 55028
output_dir: /path/to/hf
checkpoint_path: /path/to/checkpoint
save_tokenizer: false
